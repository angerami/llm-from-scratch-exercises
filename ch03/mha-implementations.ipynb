{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aeafc6b",
   "metadata": {},
   "source": [
    "# Multi-headed attnetion implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2171a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "PyTorch version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "\n",
    "batch_size= 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "embeddings = torch.randn(batch_size,context_len,embed_dim,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e24823",
   "metadata": {},
   "source": [
    "## 1. `CausalAttention` class from Ch 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d4964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "         #define the weights\n",
    "        self.W_q = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))  # New\n",
    "    \n",
    "    #now x has shape (batch, seq, feature)\n",
    "    def forward(self, x):\n",
    "        query = self.W_q(x)\n",
    "        key = self.W_k(x)\n",
    "        value = self.W_v(x)\n",
    "\n",
    "        # want (batch,nseq, nseq) <- (batch,nseq,d_out) x (batch,d_out,nseq)\n",
    "        #need transpose to swap last two rows \n",
    "        omega = query @ key.transpose(-2,-1)\n",
    "        masked = omega.masked_fill(self.mask.bool(),-torch.inf)\n",
    "        alpha = self.dropout(torch.softmax(masked/key.shape[-1]**0.5,dim=-1))\n",
    "        \n",
    "        #in value (batch,nseqj,d_out), alpha (batch,nseqi,nseqj)\n",
    "        # out (batch,nseq,d_out) \n",
    "        # keeping the order value.trasponse(-2,-1) @ alpha.transpose(-2,-1)\n",
    "        #does the right multiplication, but needs a final transpose(-2,-1) on the\n",
    "        #output to move d_out to the back.\n",
    "        #distributing this transpose over the matrix product just gives\n",
    "        context_vector = alpha @ value\n",
    "        return context_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1523fb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "class Ch03_MHA_Wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.out_proj(context_vec)\n",
    "    \n",
    "mha_ch03_wrapper = Ch03_MHA_Wrapper(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim//12,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_ch03_wrapper(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a5696",
   "metadata": {},
   "source": [
    "## 2. `MultiHeadAttention` class from Ch 3 as `Ch03_MHA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6aab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ch03_MHA(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \\\n",
    "            'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, nseq, d_in = x.shape\n",
    "        query = self.W_q(x)\n",
    "        key = self.W_k(x)\n",
    "        value = self.W_v(x)\n",
    "\n",
    "        #now split along n_heads\n",
    "        query = query.view(b,nseq,self.num_heads,self.head_dim)\n",
    "        key = key.view(b,nseq,self.num_heads,self.head_dim)\n",
    "        value = value.view(b,nseq,self.num_heads,self.head_dim)\n",
    "\n",
    "        #move n_head dimension in front of n_seq dim\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        #next is dot product attention scores, matmul over the head_dim dimension\n",
    "        # (b,n_head,n_seq,d_head) x (b,n_head,d_head,n_seq)\n",
    "        # output is (b,n_head,n_seq,n_seq')\n",
    "        omega = query @ key.transpose(-2,-1)\n",
    "\n",
    "        mask = self.mask.bool()[:nseq, :nseq]\n",
    "        omega = omega.masked_fill(mask, -torch.inf)\n",
    "        alpha = torch.softmax(omega/self.head_dim**0.5,dim = -1)\n",
    "        alpha = self.dropout(alpha)\n",
    "        # shapes are \n",
    "        # alpha ~ (b,n_head,n_seq,n_seq')\n",
    "        # value ~ (b,n_head,n_seq,d_head) \n",
    "        #context_vec ~ (b,n_head,d_seq,d_head)\n",
    "        context_vec = alpha @ value\n",
    "        #now put n_head next to d_head to roll back up\n",
    "        context_vec = context_vec.transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,nseq,self.d_out)\n",
    "\n",
    "        #now it is (b,n_seq,d_out), same shape as x except d_in -> d_out\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345d3bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "mha_ch03 = Ch03_MHA(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_ch03(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d45d3",
   "metadata": {},
   "source": [
    "## 3) `MultiHeadAttentionCombined` : an alternative multi-head attention with combined weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f066cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionCombinedQKV(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \\\n",
    "            'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "    #(b, n_seq, 3 x d)\n",
    "    def forward(self,x):\n",
    "        b, nseq, emb_dim = x.shape\n",
    "        qkv = self.W_qkv(x)\n",
    "        #now split along qkv and n_heads\n",
    "        #(b,nseq, 3, m_heads,d_out/n_heads )\n",
    "        qkv = qkv.view(b,nseq,3,self.num_heads,self.head_dim)\n",
    "        #put 3 at front and swap n_seq <-> n_heads, (3, b,n_heads,n_seq)\n",
    "        qkv = qkv.permute(2, 0 ,3 ,1, 4)\n",
    "\n",
    "        query, key, value = qkv.unbind(0)\n",
    "\n",
    "        #next is dot product attention scores, matmul over the head_dim dimension\n",
    "        # (b,n_head,n_seq,d_head) x (b,n_head,d_head,n_seq)\n",
    "        # output is (b,n_head,n_seq,n_seq')\n",
    "        omega = query @ key.transpose(-2,-1)\n",
    "\n",
    "        mask = self.mask.bool()[:nseq, :nseq]\n",
    "        omega = omega.masked_fill(mask, -torch.inf)\n",
    "        alpha = torch.softmax(omega/self.head_dim**0.5,dim = -1)\n",
    "        alpha = self.dropout(alpha)\n",
    "        # shapes are \n",
    "        # alpha ~ (b,n_head,n_seq,n_seq')\n",
    "        # value ~ (b,n_head,n_seq,d_head) \n",
    "        #context_vec ~ (b,n_head,d_seq,d_head)\n",
    "        context_vec = alpha @ value\n",
    "        #now put n_head next to d_head to roll back up\n",
    "        context_vec = context_vec.transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,nseq,self.d_out)\n",
    "\n",
    "        #now it is (b,n_seq,d_out), same shape as x except d_in -> d_out\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "mha_combined_qkv = MultiHeadAttentionCombinedQKV(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_combined_qkv(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310fb4f",
   "metadata": {},
   "source": [
    "## 4) Einsum!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e7c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class MHAEinsum(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \\\n",
    "            'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_q = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_k = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_v = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "        if qkv_bias:\n",
    "            self.bias_q = nn.Parameter(torch.zeros(d_out))\n",
    "            self.bias_k = nn.Parameter(torch.zeros(d_out))\n",
    "            self.bias_v = nn.Parameter(torch.zeros(d_out))\n",
    "\n",
    "        else:\n",
    "            self.register_parameter('bias_q', None)\n",
    "            self.register_parameter('bias_k', None)\n",
    "            self.register_parameter('bias_v', None)\n",
    "\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_k, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_v, a=math.sqrt(5))\n",
    "        if self.bias_q is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_q)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias_q, -bound, bound)\n",
    "            nn.init.uniform_(self.bias_k, -bound, bound)\n",
    "            nn.init.uniform_(self.bias_v, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, nseq, _ = x.shape\n",
    "\n",
    "        Q = torch.einsum('bnd,di->bni',x,self.W_q)\n",
    "        K = torch.einsum('bnd,di->bni',x,self.W_k)\n",
    "        V = torch.einsum('bnd,di->bni',x,self.W_v)\n",
    "\n",
    "        if self.bias_q is not None:\n",
    "            Q += self.bias_q\n",
    "            K += self.bias_k\n",
    "            V += self.bias_v\n",
    "\n",
    "        Q = Q.view(b,nseq,self.num_heads,self.head_dim)\n",
    "        K = K.view(b,nseq,self.num_heads,self.head_dim)\n",
    "        V = V.view(b,nseq,self.num_heads,self.head_dim)\n",
    "        omega = torch.einsum('bnhd,bmhd->bhnm',Q,K)\n",
    "        mask = self.mask.bool()[:nseq, :nseq]\n",
    "        omega = omega.masked_fill(mask, -torch.inf)\n",
    "        alpha = torch.softmax(omega/self.head_dim**0.5,dim = 1)\n",
    "        alpha = self.dropout(alpha)\n",
    "\n",
    "        context_vec = torch.einsum('bmhd,bhnm->bnhd',V,alpha)\n",
    "        context_vec = context_vec.contiguous().view(b,nseq,self.d_out)\n",
    "\n",
    "        #now it is (b,n_seq,d_out), same shape as x except d_in -> d_out\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "    \n",
    "mha_einsum = MHAEinsum(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_einsum(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a120382",
   "metadata": {},
   "source": [
    "## 5) PyTorch's scaled dot product attention with FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd26ceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "class MHAPyTorchScaledDotProduct(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"d_out is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.W_qkv(x)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        query, key, value = qkv.unbind(0)\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=None, dropout_p=use_dropout, is_causal=True)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.transpose(1,2).contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "mha_pytorch_scaled = MHAPyTorchScaledDotProduct(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_scaled(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a8935",
   "metadata": {},
   "source": [
    "## 6) PyTorch's scaled dot product attention w/o FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f18130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "class MHAPyTorchSDPAWithoutFlash(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \\\n",
    "            'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.dropout = dropout\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "    #(b, n_seq, 3 x d)\n",
    "    def forward(self,x):\n",
    "        b, num_tokens, emb_dim = x.shape\n",
    "        qkv = self.W_qkv(x)\n",
    "        qkv = qkv.view(b,num_tokens,3,self.num_heads,self.head_dim)\n",
    "        qkv = qkv.permute(2, 0 ,3 ,1, 4)\n",
    "        query, key, value = qkv.unbind(0)\n",
    "\n",
    "\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "\n",
    "        if self.context_length >= num_tokens:\n",
    "            attn_mask = self.mask[:num_tokens, :num_tokens]\n",
    "        else:\n",
    "            attn_mask = self.mask[:self.context_length, :self.context_length]\n",
    "\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(query, key, value,attn_mask=attn_mask, dropout_p=use_dropout, is_causal=False)\n",
    "        context_vec = context_vec.transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "mha_pytorch_sdpa_no_flash = MHAPyTorchSDPAWithoutFlash(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_sdpa_no_flash(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e113d5d",
   "metadata": {},
   "source": [
    "## 7) PyTorch's torch.nn.MultiheadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e56570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MHAPyTorchClass(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, need_weights=True):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.multiheaded_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_out,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            bias=qkv_bias,\n",
    "            add_bias_kv=qkv_bias,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.need_weights = need_weights\n",
    "        self.proj = nn.Linear(d_out,d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size, num_tokens, _ = x.shape\n",
    "\n",
    "        if self.context_length >= num_tokens:\n",
    "            attn_mask = self.mask[:num_tokens,:num_tokens]\n",
    "        else:\n",
    "            attn_mask = self.mask[:self.context_length, :self.context_length]\n",
    "\n",
    "        attn_output, _ = self.multiheaded_attn(x, x, x, attn_mask=attn_mask, need_weights=self.need_weights)\n",
    "        output = self.proj(attn_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "mha_pytorch_class_default = MHAPyTorchClass(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_class_default(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48d8d5",
   "metadata": {},
   "source": [
    "## 8) PyTorch's MultiheadAttention with scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51131bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "mha_pytorch_class_noweights = MHAPyTorchClass(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False,\n",
    "    need_weights=False # NEW!\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_class_noweights(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd8fbe",
   "metadata": {},
   "source": [
    "## 9) PyTorch's FlexAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4623cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import parse as parse_version\n",
    "\n",
    "def normalize_version(version):\n",
    "    parsed_version = parse_version(version)\n",
    "    return parse_version(f\"{parsed_version.major}.{parsed_version.minor}.{parsed_version.micro}\")\n",
    "\n",
    "current_version = normalize_version(torch.__version__)\n",
    "MIN_TORCH_VERSION = \"2.5.0\"\n",
    "required_version = parse_version(MIN_TORCH_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba9b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_version >= required_version and torch.cuda.is_available():\n",
    "    from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "\n",
    "\n",
    "def causal(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b067a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAPyTorchFlexAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \\\n",
    "            'd_out must be divisible by num_heads'\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.block_mask = create_block_mask(causal, B=None, H=None, Q_LEN=context_length, KV_LEN=context_length)\n",
    "        self.dropout = dropout\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "    #(b, n_seq, 3 x d)\n",
    "    def forward(self,x):\n",
    "        b, nseq, emb_dim = x.shape\n",
    "        qkv = self.W_qkv(x)\n",
    "        qkv = qkv.view(b,nseq,3,self.num_heads,self.head_dim)\n",
    "        qkv = qkv.permute(2, 0 ,3 ,1, 4)\n",
    "        query, key, value = qkv.unbind(0)\n",
    "\n",
    "        if self.context_length >= num_tokens:\n",
    "            attn_mask = self.block_mask[:num_tokens,:num_tokens]\n",
    "        else:\n",
    "            attn_mask = self.block_mask[:self.context_length, :self.context_length]\n",
    "\n",
    "        context_vec = flex_attention(queries, keys, values, block_mask=attn_mask)\n",
    "        context_vec = context_vec.transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vec = self.proj(context_vec)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65446a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_version >= required_version and torch.cuda.is_available():\n",
    "\n",
    "    mha_pytorch_flex = MHAPyTorchFlexAttention(\n",
    "        d_in=embed_dim,\n",
    "        d_out=embed_dim,\n",
    "        context_length=context_len,\n",
    "        dropout=0.0,\n",
    "        num_heads=12,\n",
    "        qkv_bias=False\n",
    "    ).to(device)\n",
    "\n",
    "    out = mha_pytorch_flex(embeddings)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094076ef",
   "metadata": {},
   "source": [
    "## Speed comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61f22a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n",
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0266d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 ms ± 392 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 1) CausalAttention MHA wrapper class from chapter 3\n",
    "%timeit mha_ch03_wrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb262729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.9 ms ± 650 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 2) The multi-head attention class from chapter 3\n",
    "%timeit mha_ch03(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a53a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.1 ms ± 181 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 3) An alternative multi-head attention with combined weights\n",
    "%timeit mha_combined_qkv(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "511a1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.2 ms ± 5.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 4) Multi-head attention using Einstein summation\n",
    "%timeit mha_einsum(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "495097b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ms ± 276 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 5) Multi-head attention with PyTorch's scaled dot product attention\n",
    "%timeit mha_pytorch_scaled(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946463af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.5 ms ± 790 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 6) PyTorch's scaled dot product attention without FlashAttention\n",
    "%timeit mha_pytorch_sdpa_no_flash(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 ms ± 3.52 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 7) Using PyTorch's torch.nn.MultiheadAttention\n",
    "%timeit mha_pytorch_class_default(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a3d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 ms ± 2.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 8) Using PyTorch's torch.nn.MultiheadAttention disabling `need_weights`\n",
    "%timeit mha_pytorch_class_noweights(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9) Using PyTorch's FlexAttention\n",
    "\n",
    "# Requires PyTorch 2.5.0 or newer and currently only supports CUDA PyTorch\n",
    "%timeit mha_pytorch_flex(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
